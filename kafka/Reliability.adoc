== 可靠的数据传递

* 可靠性及其在Kafka中的含义

* Kafka的复制功能，如何提高可靠性

* 根据使用场景配置Kafka的broker和主题

=== 可靠性保证

* Kafka 可以保证分区消息的顺序。

* 只有当消息被写入分区的所有同步副本时（但不一定要写入磁盘），它才被认为是“已提交”的。

* 只要还有一个副本是活跃的，那么已经提交的消息就不会丢失。

* 消费者只能读取已经提交的消息。

权衡一般是指消息存储的可靠性和一致性的重要程度与可用性、高吞吐量、低延迟和硬件成本的重要程度之间的权衡。

=== 复制

分区首领是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步的。

* 与 Zookeeper 之间有一个活跃的会话，也就是说，它在过去的 6s（可配置）内向 Zookeeper 发送过心跳。

* 在过去的 10s 内（可配置）从首领那里获取过消息。

* 在过去的 10s 内从首领那里获取过最新的消息。光从首领那里获取消息是不够的，它还必须是几乎零延迟的。

=== broker 配置

==== 复制系数

==== 不完全的首领选举

==== 最小同步副本

=== 在可靠的系统里使用生产者

* 正确的 ack

* 适当的处理错误

==== 发送确认

ack 0,1,all

==== 配置生产者的重试参数

* 重试保证消息“至少被保存一次”，不能保证“只被保存一次”。
* 处理消息的幂等。

==== 额外的错误处理

* 不可重试的 broker 错误，例如消息大小错误、认证错误等；

* 在消息发送之前发生的错误，例如序列化错误；

* 在生产者达到重试次数上限时或者在消息占用的内存达到上限时发生的错误。

如果错误处理只是为了重试发送消息，那么最好还是使用生产者内置的重试机制。

=== 在可靠的系统里使用消费者

==== 消费者的可靠性配置

.消费者可靠性配置参数
|===
|配置项 |说明

|group.id
|消费者群组 id，如果要让消费者可以看到主题所有的消息，需要设置唯一的 group.id

|auto.offset.reset
|在没有偏移量可提交时或者请求的偏移量在 broker 上不存在时,消费者的行为。earliest
-从分区的开始位置读取数据，不管偏移量是否有效，这样会导致消费者读取大量的重复数据，但可以保证最少的数据丢失；
latest-消费者会从分区的末尾开始读取数据，这样可以减少重复处理消息，但很有可能会错过一些消息。

|enable.auto.commit
|让消费者基于任务调度自动提交偏移量。主要缺点是，无法控制重复处理消息。

|auto.commit.interval.ms
|配置提交的频度，默认值5s。
|===

==== 显式提交偏移量

=== 验证系统的可靠性

==== 配置验证

org.apache.kafka.tools 中的 Verifiable Producer 和 Verifiable Consumer 可以用来测试：

* 首领选举

* 控制器选举

* 依次重启

* 不完全首领选举

==== 应用程序验证

应用程序的验证包括**检查自定义的错误处理代码、偏移量提交的方式、再均衡监听器以及其他使用了 Kafka 客户端的地方**。

建议基于如下的故障条件做一些测试：

* 客户端从服务器断开连接（系统管理员可以帮忙模拟网络故障）；

* 首领选举；

* 依次重启 broker

* 依次重启消费者

* 依次重启生产者

==== 在生产环境监控可靠性

除了监控集群的健康状况之外，监控客户端和数据流也是很重要的。

首先，Kafka 的 Java 客户端包含了 JMX 度量指标，这些指标可以用于监控客户端的状态和事件。

对于生产者来说，最重要的两个可靠性指标是消息的 error-rate 和 retry-rate（聚合过的）。

对于消费者来说，最重要的指标是 consumer-lag，该指标表明了消费者的处理速度与最近提交到分区里的偏移量之间还有多少差距。